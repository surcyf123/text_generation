MODELS_INFO = {
    "airoboros-13B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/airoboros-13B-GPTQ",
        "group_size": 128 #done
    },
    "bluemoonrp-13b": {
        "model_file": "bluemoonrp-13b-4k-epoch6-4bit-128g",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/bluemoonrp-13b",
        "group_size": 128 #done
    },
    "gpt4-x-vicuna-13B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/gpt4-x-vicuna-13B-GPTQ",
        "group_size": 128 #done
    },
    "GPT4All-13B-snoozy-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/GPT4All-13B-snoozy-GPTQ",
        "group_size": 128 #done
    },
    "Llama-2-13B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/Llama-2-13B-GPTQ",
        "group_size": 128 # pending
    },
    "Manticore-13B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/Manticore-13B-GPTQ",
        "group_size": 128 #done
    },
    "Metharme-13b-4bit-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/Metharme-13b-4bit-GPTQ",
        "group_size": 128 #done
    },
    "Nous-Hermes-13B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/Nous-Hermes-13B-GPTQ",
        "group_size": 128 #done
    },
    "guanaco-33B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/guanaco-33B-GPTQ",
        "group_size": -1 #pending
    },
    "h2ogpt-oasst1-512-30B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/h2ogpt-oasst1-512-30B-GPTQ",
        "group_size": -1,
        "unk_token": "<unk>",
        "bos_token":"<s>",
        "eos_token": "<\s>" #pending
    },
    #"tulu-30B-GPTQ": {
    #    "model_file": "model",
    #    "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/tulu-30B-GPTQ",
    #    "group_size": -1
    #},
    "WizardLM-30B-Uncensored-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/WizardLM-30B-Uncensored-GPTQ",
        "group_size": -1 #pending
    }
}



