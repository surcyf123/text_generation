MODELS_INFO = {
    "airoboros-13B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/airoboros-13B-GPTQ",
        "group_size": 128
    },
    #"bluemoonrp-13b": {
    #    "model_file": "bluemoonrp-13b-4k-epoch6-4bit-128g",
    #    "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/bluemoonrp-13b",
    #},
    "gpt4-x-vicuna-13B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/gpt4-x-vicuna-13B-GPTQ",
        "group_size": 128
    },
    "GPT4All-13B-snoozy-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/GPT4All-13B-snoozy-GPTQ",
    },
    "koala-13B-GPTQ-4bit-128g": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/koala-13B-GPTQ-4bit-128g",
        "group_size": 128
    },
    "Llama-2-13B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/Llama-2-13B-GPTQ",
        "group_size": 128
    },
    "Manticore-13B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/Manticore-13B-GPTQ",
        "group_size": 128
    },
    "Metharme-13b-4bit-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/Metharme-13b-4bit-GPTQ",
        "group_size": 128
    },
    "Nous-Hermes-13B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/Nous-Hermes-13B-GPTQ",
        "group_size": 128
    },
    "stable-vicuna-13B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/stable-vicuna-13B-GPTQ",
        "group_size": 128
    },
    #"vicuna-7B-GPTQ-4bit-128g": {
    #    "model_file": "model",
    #    "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/vicuna-7B-GPTQ-4bit-128g",
    #},
    #"open_llama_3b_4bit_128g": {
    #    "model_file": "model",
    #    "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/open_llama_3b_4bit_128g",
    #}
    "guanaco-33B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/guanaco-33B-GPTQ",
        "group_size": -1
    },
    #"h2ogpt-oasst1-512-30B-GPTQ": {
    #    "model_file": "model",
    #    "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/h2ogpt-oasst1-512-30B-GPTQ",
    #    "group_size": -1
    #},
    "tulu-30B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/tulu-30B-GPTQ",
        "group_size": -1
    },
    "WizardLM-30B-Uncensored-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/WizardLM-30B-Uncensored-GPTQ",
        "group_size": -1
    }
}



