MODELS_INFO = {
    "airoboros-13B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/airoboros-13B-GPTQ",
    },
    "bluemoonrp-13b": {
        "model_file": "bluemoonrp-13b-4k-epoch6-4bit-128g",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/bluemoonrp-13b",
    },
    "gpt4-x-vicuna-13B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/gpt4-x-vicuna-13B-GPTQ",
    },
    "GPT4All-13B-snoozy-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/GPT4All-13B-snoozy-GPTQ",
    },
    "koala-13B-GPTQ-4bit-128g": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/koala-13B-GPTQ-4bit-128g",
    },
    "Llama-2-13B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/Llama-2-13B-GPTQ",
    },
    "Manticore-13B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/Manticore-13B-GPTQ",
    },
    "Metharme-13b-4bit-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/Metharme-13b-4bit-GPTQ",
    },
    "Nous-Hermes-13B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/Nous-Hermes-13B-GPTQ",
    },
    "stable-vicuna-13B-GPTQ": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/stable-vicuna-13B-GPTQ",
    },
    "vicuna-7B-GPTQ-4bit-128g": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/vicuna-7B-GPTQ-4bit-128g",
    },
    "vicuna-7B-GPTQ-4bit-128g": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/vicuna-7B-GPTQ-4bit-128g",
    },
    "open_llama_3b_4bit_128g": {
        "model_file": "model",
        "model_dir": "/home/betogaona7/text_generation/benchmark/assets/quantized_models/open_llama_3b_4bit_128g",
    }
}



